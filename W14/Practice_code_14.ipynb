{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0736dbf-54c6-4475-a833-a3043414be16",
   "metadata": {},
   "source": [
    "### 1) NGCF model\n",
    "### 2) LightGCN model\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04b823-c673-4699-b05a-77dce606c5e8",
   "metadata": {},
   "source": [
    "### 1) NGCF class construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764a7171-38d9-4ba7-825a-b46e5bdf327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, args):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.device = args.device\n",
    "        self.emb_size = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "        self.node_dropout = args.node_dropout[0]\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        self.norm_adj = norm_adj\n",
    "\n",
    "        self.layers = eval(args.layer_size)\n",
    "        self.decay = eval(args.regs)[0]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Init the weight of user-item.\n",
    "        \"\"\"\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Get sparse adj.\n",
    "        \"\"\"\n",
    "        self.sparse_norm_adj = self._convert_sp_mat_to_sp_tensor(self.norm_adj).to(self.device)\n",
    "\n",
    "    def init_weight(self):\n",
    "        # xavier init\n",
    "        initializer = nn.init.xavier_uniform_\n",
    "\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(initializer(torch.empty(self.n_user,\n",
    "                                                 self.emb_size))),\n",
    "            'item_emb': nn.Parameter(initializer(torch.empty(self.n_item,\n",
    "                                                 self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layers\n",
    "        for k in range(len(self.layers)):\n",
    "            weight_dict.update({'W_gc_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_gc_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "            weight_dict.update({'W_bi_%d'%k: nn.Parameter(initializer(torch.empty(layers[k],\n",
    "                                                                      layers[k+1])))})\n",
    "            weight_dict.update({'b_bi_%d'%k: nn.Parameter(initializer(torch.empty(1, layers[k+1])))})\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo()\n",
    "        i = torch.LongTensor([coo.row, coo.col])\n",
    "        v = torch.from_numpy(coo.data).float()\n",
    "        return torch.sparse.FloatTensor(i, v, coo.shape)\n",
    "\n",
    "    def sparse_dropout(self, x, rate, noise_shape):\n",
    "        random_tensor = 1 - rate\n",
    "        random_tensor += torch.rand(noise_shape).to(x.device)\n",
    "        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n",
    "        i = x._indices()\n",
    "        v = x._values()\n",
    "\n",
    "        i = i[:, dropout_mask]\n",
    "        v = v[dropout_mask]\n",
    "\n",
    "        out = torch.sparse.FloatTensor(i, v, x.shape).to(x.device)\n",
    "        return out * (1. / (1 - rate))\n",
    "\n",
    "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        # cul regularizer\n",
    "        regularizer = (torch.norm(users) ** 2\n",
    "                       + torch.norm(pos_items) ** 2\n",
    "                       + torch.norm(neg_items) ** 2) / 2\n",
    "        emb_loss = self.decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss, mf_loss, emb_loss\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items, drop_flag=True):\n",
    "\n",
    "        A_hat = self.sparse_dropout(self.sparse_norm_adj,\n",
    "                                    self.node_dropout,\n",
    "                                    self.sparse_norm_adj._nnz()) if drop_flag else self.sparse_norm_adj\n",
    "\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'],\n",
    "                                    self.embedding_dict['item_emb']], 0)\n",
    "\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(len(self.layers)):\n",
    "            side_embeddings = torch.sparse.mm(A_hat, ego_embeddings)\n",
    "\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = torch.matmul(side_embeddings, self.weight_dict['W_gc_%d' % k]) \\\n",
    "                                             + self.weight_dict['b_gc_%d' % k]\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            # element-wise product\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_bi_%d' % k]) \\\n",
    "                                            + self.weight_dict['b_bi_%d' % k]\n",
    "\n",
    "            # non-linear activation.\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "\n",
    "            # message dropout.\n",
    "            ego_embeddings = nn.Dropout(self.mess_dropout[k])(ego_embeddings)\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        look up.\n",
    "        \"\"\"\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103f64f-1fb6-4d17-8b61-5394d18ad257",
   "metadata": {},
   "source": [
    "### Data object construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a63c87-f02a-43bb-a1c3-fa468e373a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    self.exist_users.append(uid)\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_users = max(self.n_users, uid)\n",
    "                    self.n_train += len(items)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    self.n_items = max(self.n_items, max(items))\n",
    "                    self.n_test += len(items)\n",
    "        self.n_items += 1\n",
    "        self.n_users += 1\n",
    "\n",
    "        self.print_statistics()\n",
    "\n",
    "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        with open(train_file) as f_train:\n",
    "            with open(test_file) as f_test:\n",
    "                for l in f_train.readlines():\n",
    "                    if len(l) == 0:\n",
    "                        break\n",
    "                    l = l.strip('\\n')\n",
    "                    items = [int(i) for i in l.split(' ')]\n",
    "                    uid, train_items = items[0], items[1:]\n",
    "\n",
    "                    for i in train_items:\n",
    "                        self.R[uid, i] = 1.\n",
    "                        # self.R[uid][i] = 1\n",
    "\n",
    "                    self.train_items[uid] = train_items\n",
    "\n",
    "                for l in f_test.readlines():\n",
    "                    if len(l) == 0: break\n",
    "                    l = l.strip('\\n')\n",
    "                    try:\n",
    "                        items = [int(i) for i in l.split(' ')]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    uid, test_items = items[0], items[1:]\n",
    "                    self.test_set[uid] = test_items\n",
    "\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n",
    "            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n",
    "            print('already load adj matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        except Exception:\n",
    "            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n",
    "            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n",
    "        return adj_mat, norm_adj_mat, mean_adj_mat\n",
    "\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R.tolil()\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        def mean_adj_single(adj):\n",
    "            # D^-1 * A\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            # norm_adj = adj.dot(d_mat_inv)\n",
    "            print('generate single-normalized adjacency matrix.')\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        def normalized_adj_single(adj):\n",
    "            # D^-1/2 * A * D^-1/2\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            # bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            bi_lap = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def check_adj_if_equal(adj):\n",
    "            dense_A = np.array(adj.todense())\n",
    "            degree = np.sum(dense_A, axis=1, keepdims=False)\n",
    "\n",
    "            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n",
    "            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n",
    "            return temp\n",
    "\n",
    "        norm_adj_mat = mean_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        # norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n",
    "        mean_adj_mat = mean_adj_single(adj_mat)\n",
    "\n",
    "        print('already normalize adjacency matrix', time() - t2)\n",
    "        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n",
    "\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            # sample num pos items for u-th user\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num:\n",
    "                    break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            # sample num neg items for u-th user\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num:\n",
    "                    break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_set.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_items[uid]\n",
    "            test_iids = self.test_set[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "\n",
    "\n",
    "        return split_uids, split_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e47d1-4f07-4e53-9964-61bfee427ef6",
   "metadata": {},
   "source": [
    "### Metrics: \n",
    "\n",
    "precision_at_k, average_precision, mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200e6638-ea12-488b-9e98-34f9bfda3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def recall(rank, ground_truth, N):\n",
    "    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, ground_truth, method=1):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "\n",
    "        Low but correct defination\n",
    "    \"\"\"\n",
    "    GT = set(ground_truth)\n",
    "    if len(GT) > k :\n",
    "        sent_list = [1.0] * k\n",
    "    else:\n",
    "        sent_list = [1.0]*len(GT) + [0.0]*(k-len(GT))\n",
    "    dcg_max = dcg_at_k(sent_list, k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    # if all_pos_num == 0:\n",
    "    #     return 0\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num\n",
    "\n",
    "\n",
    "def hit_at_k(r, k):\n",
    "    r = np.array(r)[:k]\n",
    "    if np.sum(r) > 0:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def AUC(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n",
    "    except Exception:\n",
    "        res = 0.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a57b3-c6c5-47bc-8f12-2aa5f9dee887",
   "metadata": {},
   "source": [
    "### change to NGCF folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780aab82-59fd-453b-9db9-9505bde4e2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Class\\GNNs\\W14\n",
      "C:\\Users\\user\\Class\\GNNs\\W14\\NGCF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd()+ '//NGCF')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ea1d8-5aee-42b9-918a-88691559f459",
   "metadata": {},
   "source": [
    "### Run the main.py code\n",
    "\n",
    "Open Visual Studio/Pycharm to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1848405-0d11-48f1-9425-434e3c21eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('python main.py --dataset gowalla --regs [1e-5] --embed_size 64 --layer_size [64,64,64] --lr 0.0001 --save_flag 1 --pretrain 0 --batch_size 1024 --epoch 400 --verbose 1 --node_dropout [0.1] --mess_dropout [0.1,0.1,0.1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528566c-70bd-4051-84e2-6fd5254c4684",
   "metadata": {},
   "source": [
    "### 2) LightGCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600a5421-63e4-459e-987c-60ba1d849d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\GNN16\\lib\\site-packages\\torch_geometric\\typing.py:63: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "C:\\Users\\user\\anaconda3\\envs\\GNN16\\lib\\site-packages\\torch_geometric\\typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import AmazonBook\n",
    "from torch_geometric.nn import LightGCN\n",
    "from torch_geometric.utils import degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34651580-83d0-4c5d-b3d9-0302faa4d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/gusye1234/LightGCN-PyTorch/master/data/amazon-book/user_list.txt\n",
      "Downloading https://raw.githubusercontent.com/gusye1234/LightGCN-PyTorch/master/data/amazon-book/item_list.txt\n",
      "Downloading https://raw.githubusercontent.com/gusye1234/LightGCN-PyTorch/master/data/amazon-book/train.txt\n",
      "Downloading https://raw.githubusercontent.com/gusye1234/LightGCN-PyTorch/master/data/amazon-book/test.txt\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataset = AmazonBook(\"Amazon\")\n",
    "data = dataset[0]\n",
    "num_users, num_books = data['user'].num_nodes, data['book'].num_nodes\n",
    "data = data.to_homogeneous().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d00a30-e37e-4ca4-8a95-fd840b6f3e03",
   "metadata": {},
   "source": [
    "### Use all message passing edges as training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edef5f40-53ca-426b-9b6e-f44e50ad39e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "batch_size = 8192\n",
    "mask = data.edge_index[0] < data.edge_index[1]\n",
    "train_edge_label_index = data.edge_index[:, mask]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    range(train_edge_label_index.size(1)),\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd80c3-acc6-4a77-adc8-4f3ec1acd490",
   "metadata": {},
   "source": [
    "### LightGCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e20201b-577e-4974-9ad5-4162f8253522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LightGCN(\n",
    "    num_nodes=data.num_nodes,\n",
    "    embedding_dim=64,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646cf6e-d252-49d6-89d5-cde97c3a45dc",
   "metadata": {},
   "source": [
    "### train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4671df54-f295-409b-8e03-9aa23f171edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    total_loss = total_examples = 0\n",
    "\n",
    "    for index in tqdm(train_loader):\n",
    "        # Sample positive and negative labels.\n",
    "        pos_edge_label_index = train_edge_label_index[:, index]\n",
    "        neg_edge_label_index = torch.stack([\n",
    "            pos_edge_label_index[0],\n",
    "            torch.randint(num_users, num_users + num_books,\n",
    "                          (index.numel(), ), device=device)\n",
    "        ], dim=0)\n",
    "        edge_label_index = torch.cat([\n",
    "            pos_edge_label_index,\n",
    "            neg_edge_label_index,\n",
    "        ], dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pos_rank, neg_rank = model(data.edge_index, edge_label_index).chunk(2)\n",
    "\n",
    "        loss = model.recommendation_loss(\n",
    "            pos_rank,\n",
    "            neg_rank,\n",
    "            node_id=edge_label_index.unique(),\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * pos_rank.numel()\n",
    "        total_examples += pos_rank.numel()\n",
    "\n",
    "    return total_loss / total_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb051ee4-faab-4e1e-8e78-92a82c7c04fa",
   "metadata": {},
   "source": [
    "### test function at @k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0559f12-817a-45ba-9dc9-e73e1782eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(k: int):\n",
    "    emb = model.get_embedding(data.edge_index)\n",
    "    user_emb, book_emb = emb[:num_users], emb[num_users:]\n",
    "\n",
    "    precision = recall = total_examples = 0\n",
    "    for start in range(0, num_users, batch_size):\n",
    "        end = start + batch_size\n",
    "        logits = user_emb[start:end] @ book_emb.t()\n",
    "\n",
    "        # Exclude training edges:\n",
    "        mask = ((train_edge_label_index[0] >= start) &\n",
    "                (train_edge_label_index[0] < end))\n",
    "        logits[train_edge_label_index[0, mask] - start,\n",
    "               train_edge_label_index[1, mask] - num_users] = float('-inf')\n",
    "\n",
    "        # Computing precision and recall:\n",
    "        ground_truth = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        mask = ((data.edge_label_index[0] >= start) &\n",
    "                (data.edge_label_index[0] < end))\n",
    "        ground_truth[data.edge_label_index[0, mask] - start,\n",
    "                     data.edge_label_index[1, mask] - num_users] = True\n",
    "        node_count = degree(data.edge_label_index[0, mask] - start,\n",
    "                            num_nodes=logits.size(0))\n",
    "\n",
    "        topk_index = logits.topk(k, dim=-1).indices\n",
    "        isin_mat = ground_truth.gather(1, topk_index)\n",
    "\n",
    "        precision += float((isin_mat.sum(dim=-1) / k).sum())\n",
    "        recall += float((isin_mat.sum(dim=-1) / node_count.clamp(1e-6)).sum())\n",
    "        total_examples += int((node_count > 0).sum())\n",
    "\n",
    "    return precision / total_examples, recall / total_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c75691-55f6-4f45-9ab8-b097b44182c4",
   "metadata": {},
   "source": [
    "### Runnung epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb723d7-d7b1-463b-8903-e64a265f255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [00:13<00:00, 22.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.2303, Precision@20: 0.0070, Recall@20: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [00:12<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.2058, Precision@20: 0.0075, Recall@20: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [00:13<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.1877, Precision@20: 0.0078, Recall@20: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 291/291 [00:13<00:00, 22.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.1739, Precision@20: 0.0082, Recall@20: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, 5):\n",
    "    loss = train()\n",
    "    precision, recall = test(k=20)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Precision@20: '\n",
    "          f'{precision:.4f}, Recall@20: {recall:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
