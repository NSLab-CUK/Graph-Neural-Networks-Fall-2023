{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4019db76-2e38-413a-809d-05eae972814d",
   "metadata": {},
   "source": [
    "## 1. GraphSage sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9de92ec-0d05-4011-84c7-2a189d760e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (1.3.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (1.24.4)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (1.3.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (2.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (2.0.4)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from ogb) (0.2.2)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from outdated>=0.2.0->ogb) (68.0.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from torch>=1.6.0->ogb) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from torch>=1.6.0->ogb) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from torch>=1.6.0->ogb) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from tqdm>=4.29.0->ogb) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (0.58.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (0.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from umap-learn) (4.66.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.41.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from numba>=0.51.2->umap-learn) (6.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from tqdm->umap-learn) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from importlib-metadata->numba>=0.51.2->umap-learn) (3.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from seaborn) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\gnn5\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing Pytorch Geometric \n",
    "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "!pip install -q torch-geometric\n",
    "!pip install ogb\n",
    "!pip install umap-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6090833-d736-4ecf-97b8-a49f941e3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from pandas.core.common import flatten\n",
    "# importing obg datatset\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from pandas.core.common import flatten\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
    "sns.set_theme(style=\"ticks\")\n",
    "import collections\n",
    "from scipy.special import softmax\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9c4910-82cf-4245-9b53-0393ca8b5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and loading the obg dataset\n",
    "path = osp.join('data', 'Reddit')\n",
    "dataset = PygNodePropPredDataset('ogbn-products', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79c7bee-0a67-4fb3-b9aa-150c35ea6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_idx contains a dictionary of train, validation and test node indices\n",
    "split_idx = dataset.get_idx_split()\n",
    "# predefined ogb evaluator method used for validation of predictions\n",
    "evaluator = Evaluator(name='ogbn-products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6c7942-b7a6-4bd4-b06c-10403be64c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training nodes: 196615\n",
      "Number of validation nodes: 39323\n",
      "Number of test nodes: 2213091\n"
     ]
    }
   ],
   "source": [
    "# lets check the node ids distribution of train, test and val\n",
    "print('Number of training nodes:', split_idx['train'].size(0))\n",
    "print('Number of validation nodes:', split_idx['valid'].size(0))\n",
    "print('Number of test nodes:', split_idx['test'].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02411752-e3b6-4a03-b260-439c5aa82d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f5d3a2-8c8a-472a-85a4-21e85dbd7496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 2449029\n",
      "Number of edges in the graph: 123718280\n",
      "Node feature matrix with shape: torch.Size([2449029, 100])\n",
      "Graph connectivity in COO format with shape: torch.Size([2, 123718280])\n",
      "Target to train against : torch.Size([2449029, 1])\n",
      "Node feature length 100\n"
     ]
    }
   ],
   "source": [
    "# graph statistics of ogb-product graph\n",
    "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
    "print(\"Number of edges in the graph:\", data.num_edges)\n",
    "print(\"Node feature matrix with shape:\", data.x.shape) # [num_nodes, num_node_features]\n",
    "print(\"Graph connectivity in COO format with shape:\", data.edge_index.shape) # [2, num_edges]\n",
    "print(\"Target to train against :\", data.y.shape) \n",
    "print(\"Node feature length\", dataset.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb031562-9848-4db0-a846-6668763cba51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of unique labels\n",
    "# there are 47 unique categories of product\n",
    "data.y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b033e3-ac9f-453b-9a9d-c47b6a80f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load integer to real product category from label mapping provided inside the dataset\n",
    "df = pd.read_csv('data/Reddit/ogbn_products/mapping/labelidx2productcategory.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57c094b-eccc-439c-9f44-012d2fef9a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a dictionary of product category and corresponding integer label\n",
    "label_idx, prod_cat = df.iloc[: ,0].values, df.iloc[: ,1].values\n",
    "label_mapping = dict(zip(label_idx, prod_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a262df-1226-421e-bac3-28592f12e020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label idx</th>\n",
       "      <th>product category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Health &amp; Personal Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Patio, Lawn &amp; Garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>CDs &amp; Vinyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Cell Phones &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Grocery &amp; Gourmet Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label idx           product category\n",
       "0          0             Home & Kitchen\n",
       "1          1     Health & Personal Care\n",
       "2          2                     Beauty\n",
       "3          3          Sports & Outdoors\n",
       "4          4                      Books\n",
       "5          5       Patio, Lawn & Garden\n",
       "6          6               Toys & Games\n",
       "7          7                CDs & Vinyl\n",
       "8          8  Cell Phones & Accessories\n",
       "9          9     Grocery & Gourmet Food"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see some of the product categories\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e34adfa8-a518-4807-90eb-d5f77cbad8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 668950, 7: 172199, 6: 158771, 3: 151061, 12: 131886, 2: 116043, 0: 114294, 8: 110796, 1: 109832, 13: 101541, 16: 83594, 21: 80795, 9: 67358, 10: 52345, 18: 49019, 24: 45406, 17: 42337, 5: 40715, 11: 32937, 42: 32500, 15: 26911, 20: 22575, 19: 17438, 23: 3653, 14: 3079, 25: 3024, 28: 1969, 29: 1561, 43: 1399, 22: 879, 36: 630, 44: 566, 26: 553, 37: 514, 32: 513, 31: 418, 30: 277, 27: 259, 34: 154, 38: 91, 41: 61, 35: 44, 39: 37, 33: 29, 45: 9, 40: 6, 46: 1})\n"
     ]
    }
   ],
   "source": [
    "# counting the numbers of samples for each category\n",
    "y = data.y.tolist()\n",
    "y = list(flatten(y))\n",
    "count_y = collections.Counter(y)\n",
    "print(count_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa9477f-66e5-4afc-a81f-754c419c7284",
   "metadata": {},
   "source": [
    "### Neighborhood Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b863d39c-23e1-4882-8bf9-4613e0ff2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\GNN5\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_idx = split_idx['train']\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "                               sizes=[15, 10, 5], batch_size=1024,\n",
    "                               shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d0e695-f762-46c0-9532-d153fbcfec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n",
    "        # and returns, for each layer, a bipartite graph object, holding the\n",
    "        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n",
    "        # and the size/shape `size` of the bipartite graph.\n",
    "        # Target nodes are also included in the source nodes so that one can\n",
    "        # easily apply skip-connections or add self-loops.\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            xs = []\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "            xs.append(x)\n",
    "            if i == 0: \n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_1_embeddings = x_all\n",
    "            elif i == 1:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_2_embeddings = x_all\n",
    "            elif i == 2:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_3_embeddings = x_all    \n",
    "        #return x.log_softmax(dim=-1)\n",
    "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        total_edges = 0\n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                total_edges += edge_index.size(1)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                if i != self.num_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x)\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            if i == 0: \n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_1_embeddings = x_all\n",
    "            elif i == 1:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_2_embeddings = x_all\n",
    "            elif i == 2:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_3_embeddings = x_all\n",
    "                \n",
    "        pbar.close()\n",
    "\n",
    "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "420c4fec-cb04-453b-a434-ef1bf8bbad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes, num_layers=3)\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ec63be-8504-4a26-9d09-d60cb8cf0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading node feature matrix and node labels\n",
    "x = data.x.to(device)\n",
    "y = data.y.squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66799035-6c02-4f0a-b0a2-9f200fb5d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        optimizer.zero_grad()    \n",
    "        l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)\n",
    "        #print(\"Layer 1 embeddings\", l1_emb.shape)\n",
    "        #print(\"Layer 2 embeddings\", l1_emb.shape)\n",
    "        out = l3_emb.log_softmax(dim=-1)\n",
    "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
    "        #pbar.update(batch_size)\n",
    "\n",
    "    #pbar.close()\n",
    "\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / train_idx.size(0)\n",
    "\n",
    "    return loss, approx_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24004827-2c91-4e05-abae-7c139142dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 0.6486, Approx. Train: 0.8308\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    print('Processing ...')\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ddfc328-b186-4058-9c47-d42f27ac4f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAGE(\n",
       "  (convs): ModuleList(\n",
       "    (0): SAGEConv(100, 256, aggr=mean)\n",
       "    (1): SAGEConv(256, 256, aggr=mean)\n",
       "    (2): SAGEConv(256, 47, aggr=mean)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6cc54a4-be88-4b79-afa2-a364b8b70ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "convs.0.lin_l.weight \t torch.Size([256, 100])\n",
      "convs.0.lin_l.bias \t torch.Size([256])\n",
      "convs.0.lin_r.weight \t torch.Size([256, 100])\n",
      "convs.1.lin_l.weight \t torch.Size([256, 256])\n",
      "convs.1.lin_l.bias \t torch.Size([256])\n",
      "convs.1.lin_r.weight \t torch.Size([256, 256])\n",
      "convs.2.lin_l.weight \t torch.Size([47, 256])\n",
      "convs.2.lin_l.bias \t torch.Size([47])\n",
      "convs.2.lin_r.weight \t torch.Size([47, 256])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f00c7e4-39f0-4a9f-8215-f3dac723037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "fp = 'data/model.pt'\n",
    "\n",
    "torch.save(model, './model.pt')\n",
    "torch.save(model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ae245-c23e-4d30-9ba1-e50cd6a01b36",
   "metadata": {},
   "source": [
    "## 2. Graph saint model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4abaa07-bcef-4183-8ffd-3318c5b1be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Flickr, Planetoid\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.typing import WITH_TORCH_SPARSE\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14576457-f8ac-4867-8570-d551bed0f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('data', 'cora')\n",
    "dataset = Planetoid(path,name = 'cora')\n",
    "data = dataset[0]\n",
    "row, col = data.edge_index\n",
    "data.edge_weight = 1. / degree(col, data.num_nodes)[col]  # Norm by in-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d5d16c-f34c-4959-aaa2-7fd51dd24aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the graph: 2708\n",
      "Number of edges in the graph: 10556\n",
      "Node feature matrix with shape: torch.Size([2708, 1433])\n",
      "Graph connectivity in COO format with shape: torch.Size([2, 10556])\n",
      "Target to train against : torch.Size([2708])\n",
      "Node feature length 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute GraphSAINT normalization:  16%|█████▌                            | 4034391/24490290 [04:50<23:22, 14582.64it/s]"
     ]
    }
   ],
   "source": [
    "# lets check some graph statistics of graph\n",
    "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
    "print(\"Number of edges in the graph:\", data.num_edges)\n",
    "print(\"Node feature matrix with shape:\", data.x.shape) # [num_nodes, num_node_features]\n",
    "print(\"Graph connectivity in COO format with shape:\", data.edge_index.shape) # [2, num_edges]\n",
    "print(\"Target to train against :\", data.y.shape) \n",
    "print(\"Node feature length\", dataset.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a688058-96d4-442c-b638-30ac64cd37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--use_normalization', required=False, action='store_true')\n",
    "parser.add_argument(\"-f\", required=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "loader = GraphSAINTRandomWalkSampler(data, batch_size=16, walk_length=2,\n",
    "                                     num_steps=5, sample_coverage=10,\n",
    "                                     save_dir=dataset.processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cc9e48b-4219-47ba-92cb-1793cf35330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        in_channels = dataset.num_node_features\n",
    "        out_channels = dataset.num_classes\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(3 * hidden_channels, out_channels)\n",
    "\n",
    "    def set_aggr(self, aggr):\n",
    "        self.conv1.aggr = aggr\n",
    "        self.conv2.aggr = aggr\n",
    "        self.conv3.aggr = aggr\n",
    "\n",
    "    def forward(self, x0, edge_index, edge_weight=None):\n",
    "        x1 = F.relu(self.conv1(x0, edge_index, edge_weight))\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = F.relu(self.conv2(x1, edge_index, edge_weight))\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        x3 = F.relu(self.conv3(x2, edge_index, edge_weight))\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        x = torch.cat([x1, x2, x3], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        return x.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0c16e7c-e18a-4b34-b505-2b9681b533b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(hidden_channels=256).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f69c5d17-3e00-401d-8a01-160599bf640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    model.set_aggr('add' if args.use_normalization else 'mean')\n",
    "\n",
    "    total_loss = total_examples = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if args.use_normalization:\n",
    "            edge_weight = data.edge_norm * data.edge_weight\n",
    "            out = model(data.x, data.edge_index, edge_weight)\n",
    "            loss = F.nll_loss(out, data.y, reduction='none')\n",
    "            loss = (loss * data.node_norm)[data.train_mask].sum()\n",
    "        else:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_nodes\n",
    "        total_examples += data.num_nodes\n",
    "    return total_loss / total_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e245a98-3772-48c9-bd85-e02779a1b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    model.set_aggr('mean')\n",
    "\n",
    "    out = model(data.x.to(device), data.edge_index.to(device))\n",
    "    pred = out.argmax(dim=-1)\n",
    "    correct = pred.eq(data.y.to(device))\n",
    "\n",
    "    accs = []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        accs.append(correct[mask].sum().item() / mask.sum().item())\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3090ede5-52d4-420b-b82a-e71a7dbf0338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.9560, Train: 0.2643, Val: 0.1560, Test: 0.1670\n",
      "Epoch: 02, Loss: nan, Train: 0.1500, Val: 0.0740, Test: 0.0920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, 3):\n",
    "    loss = train()\n",
    "    accs = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {accs[0]:.4f}, '\n",
    "          f'Val: {accs[1]:.4f}, Test: {accs[2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26aa83-6373-49cc-b1e2-6431e038d65a",
   "metadata": {},
   "source": [
    "### Cluster_GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e4459d7-5140-468b-a04a-506cda72d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader, NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94ec5f76-e776-4213-9aec-c850049e91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "cluster_data = ClusterData(data, num_parts=10, recursive=False, save_dir=dataset.processed_dir)\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=20, shuffle=True,\n",
    "                             num_workers=12)\n",
    "\n",
    "subgraph_loader = NeighborLoader(data, num_neighbors=[-1], batch_size=1024,\n",
    "                                 shuffle=False, num_workers=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0654ed4-d48a-426c-a0a5-ec22e54eef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = ModuleList(\n",
    "            [SAGEConv(in_channels, 128),\n",
    "             SAGEConv(128, out_channels)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch.\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                edge_index = batch.edge_index.to(device)\n",
    "                x = x_all[batch.n_id].to(device)\n",
    "                x_target = x[:batch.batch_size]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch.batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59558d-33b2-4eb0-95de-8c9dc8321350",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d684732d-a313-4f59-80d9-b32d2bf81fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55d01337-737f-45b1-9e0f-0e8941dcd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "017c7e99-41da-40d5-b0f8-df5cbb3a774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test():  # Inference should be performed on the full graph.\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "785ecb98-67ea-4139-847a-592cafe20154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.4567\n",
      "Epoch: 02, Loss: 1.1270\n",
      "Epoch: 03, Loss: 0.7796\n",
      "Epoch: 04, Loss: 0.5385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/5416 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   0%|                                                                             | 0/5416 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  19%|████████████▎                                                    | 1024/5416 [00:03<00:15, 277.73it/s]\u001b[A\n",
      "Evaluating:  38%|████████████████████████▌                                        | 2048/5416 [00:03<00:05, 627.11it/s]\u001b[A\n",
      "Evaluating:  69%|████████████████████████████████████████████▊                    | 3732/5416 [00:08<00:03, 423.79it/s]\u001b[A\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████| 5416/5416 [00:10<00:00, 528.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3032, Train: 1.0000, Val: 0.7720, test: 0.7850\n",
      "Epoch: 06, Loss: 0.1864\n",
      "Epoch: 07, Loss: 0.0986\n",
      "Epoch: 08, Loss: 0.0600\n",
      "Epoch: 09, Loss: 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/5416 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   0%|                                                                             | 0/5416 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:  19%|████████████▎                                                    | 1024/5416 [00:03<00:15, 282.95it/s]\u001b[A\n",
      "Evaluating:  38%|████████████████████████▌                                        | 2048/5416 [00:03<00:05, 640.87it/s]\u001b[A\n",
      "Evaluating:  69%|████████████████████████████████████████████▊                    | 3732/5416 [00:08<00:04, 420.21it/s]\u001b[A\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████| 5416/5416 [00:10<00:00, 522.58it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.0167, Train: 1.0000, Val: 0.7740, test: 0.7920\n",
      "Epoch: 11, Loss: 0.0106\n",
      "Epoch: 12, Loss: 0.0066\n",
      "Epoch: 13, Loss: 0.0040\n",
      "Epoch: 14, Loss: 0.0032\n",
      "Median time per epoch: 5.0810s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = []\n",
    "for epoch in range(1,15):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "              f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59046cb-959c-4aef-9ddf-29f858bdb391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
