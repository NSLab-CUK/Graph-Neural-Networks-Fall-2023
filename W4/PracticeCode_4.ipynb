{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca317852c42a0515",
   "metadata": {},
   "source": [
    "# 1. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5c03c-f911-45cd-8eea-e2ebefc39533",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install matplotlib\n",
    "!pip install -U scikit-learn\n",
    "!pip install torch_geometric\n",
    "!pip install networkx\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.1+cpu.html\n",
    "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.1+cpu.html\n",
    "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-2.0.1+cpu.html\n",
    "!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.0.1+cpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17bb24887c4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.datasets.planetoid import Planetoid\n",
    "from torch_geometric.datasets.aminer import AMiner\n",
    "from torch_geometric.utils import to_networkx, to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9a3295cfa390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cora Dataset for GraRep\n",
    "cora_data = Planetoid(root='/tmp/Cora', name='Cora')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b85fe999f457e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cb64d62c27f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_A = to_dense_adj(cora_data.edge_index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e8e63251b00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9a8ac39bf81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMiner Dataset for Metapath2vec\n",
    "!pip install pandas\n",
    "aminer_data = AMiner(\"./\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071f521cf8a2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "aminer_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb6b81",
   "metadata": {},
   "source": [
    "# 2. An example of Computing Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "nx_g = nx.Graph()\n",
    "\n",
    "nx_g.add_edges_from([(0, 1), (0, 2),  (2, 3), (2, 4), (3,5 ), (0,3), (0,5), (1,5), (3,6), (2,5), (1,7),(8,7), (3,9)])\n",
    "nx.draw(nx_g,with_labels = True)\n",
    "num_nodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ak = np.matrix(np.identity(num_nodes))\n",
    "adj = nx.adjacency_matrix(nx_g)\n",
    "adj= adj.todense()\n",
    "adj = normalize(adj, axis=1, norm='l1')\n",
    "adj = np.round(adj,2)\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4525746",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    Ak = np.dot(Ak, adj)\n",
    "    print(f'Step: {i+1}')\n",
    "    print(np.round(Ak,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5cd951d51111a4",
   "metadata": {},
   "source": [
    "# 3. Grarep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f608877158f33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraRep(object):\n",
    "    def __init__(self, A, args):\n",
    "        \"\"\"\n",
    "        :param A: Adjacency matrix.\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.args = args\n",
    "        self._setup_base_target_matrix()\n",
    "\n",
    "    def _setup_base_target_matrix(self):\n",
    "        \"\"\"\n",
    "        Creating a base matrix to multiply.\n",
    "        \"\"\"\n",
    "        values = [1.0 for _ in range(self.A.shape[0])]\n",
    "        indices = [i for i in range(self.A.shape[0])]\n",
    "        self.A_hat = sparse.coo_matrix((values, (indices, indices)),\n",
    "                                       shape=self.A.shape,\n",
    "                                       dtype=np.float32)\n",
    "\n",
    "    def _create_target_matrix(self):\n",
    "        \"\"\"\n",
    "        Creating a log transformed target matrix.\n",
    "        :return target_matrix: Matrix to decompose with SVD.\n",
    "        \"\"\"\n",
    "        self.A_hat = sparse.coo_matrix(self.A_hat.dot(self.A))\n",
    "        scores = np.log(self.A_hat.data)-math.log(self.A.shape[0])\n",
    "        rows = self.A_hat.row[scores < 0]\n",
    "        cols = self.A_hat.col[scores < 0]\n",
    "        scores = scores[scores < 0]\n",
    "        target_matrix = sparse.coo_matrix((scores, (rows, cols)),\n",
    "                                          shape=self.A.shape,\n",
    "                                          dtype=np.float32)\n",
    "        return target_matrix\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Learning an embedding.\n",
    "        \"\"\"\n",
    "        print(\"\\nOptimization started.\\n\")\n",
    "        self.embeddings = []\n",
    "        for step in tqdm(range(self.args[\"order\"])):\n",
    "            target_matrix = self._create_target_matrix()\n",
    "\n",
    "            svd = TruncatedSVD(n_components=self.args[\"dim\"],\n",
    "                               n_iter=self.args[\"iter\"],\n",
    "                               random_state=self.args[\"seed\"])\n",
    "\n",
    "            svd.fit(target_matrix)\n",
    "            embedding = svd.transform(target_matrix)\n",
    "            self.embeddings.append(embedding)\n",
    "\n",
    "    def return_embedding(self):\n",
    "        \"\"\"\n",
    "        Return the embedding.\n",
    "        \"\"\"\n",
    "        print(\"\\nReturn embedding.\\n\")\n",
    "        self.embeddings = np.concatenate(self.embeddings, axis=1)\n",
    "        column_count = self.args[\"order\"] * self.args[\"dim\"]\n",
    "        columns = [\"ID\"] + [\"x_\" + str(col) for col in range(column_count)]\n",
    "        ids = np.array([i for i in range(self.A.shape[0])]).reshape(-1,1)\n",
    "        self.embeddings = np.concatenate([ids, self.embeddings], axis=1)\n",
    "        self.embeddings = pd.DataFrame(self.embeddings, columns=columns)\n",
    "        \n",
    "        return self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a59ea984b1429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dim\": 16,\n",
    "    \"order\": 5,\n",
    "    \"seed\": 42,\n",
    "    \"iter\": 20\n",
    "}\n",
    "\n",
    "grarep = GraRep(cora_A, args)\n",
    "\n",
    "grarep.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf97ea4761029a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grarep_emb_csv = grarep.return_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bfcd95f5a99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grarep_emb_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. SDNE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccbb95aef8c01787"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3661e0caa73a058"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SDNE(nn.Module):\n",
    "    def __init__(self, node_size, nhid0, nhid1, dropout, alpha):\n",
    "        \"\"\"\n",
    "        :param node_size: the size of node\n",
    "        :param nhid0: the input vector size\n",
    "        :param nhid1: the output vector size\n",
    "        :param dropout: the value for dropout layer\n",
    "        :param alpha: the value for loss\n",
    "        \"\"\"\n",
    "        super(SDNE, self).__init__()\n",
    "        self.encode0 = nn.Linear(node_size, nhid0)\n",
    "        self.encode1 = nn.Linear(nhid0, nhid1)\n",
    "        self.decode0 = nn.Linear(nhid1, nhid0)\n",
    "        self.decode1 = nn.Linear(nhid0, node_size)\n",
    "        self.droput = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, adj_batch, adj_mat, b_mat):\n",
    "        \"\"\"\n",
    "        :param adj_batch: [batch_size, the number of node] => adj matrix for node in batch\n",
    "        :param adj_mat: [batch_size, batch_size] => adj matrixf for node only in batch\n",
    "        :param b_mat: \n",
    "        :return: B matrix for L_2nd\n",
    "        \"\"\"\n",
    "        t0 = F.leaky_relu(self.encode0(adj_batch))\n",
    "        t0 = F.leaky_relu(self.encode1(t0))\n",
    "        embedding = t0\n",
    "        t0 = F.leaky_relu(self.decode0(t0))\n",
    "        t0 = F.leaky_relu(self.decode1(t0))\n",
    "        embedding_norm = torch.sum(embedding * embedding, dim=1, keepdim=True)\n",
    "        L_1st = torch.sum(adj_mat * (embedding_norm -\n",
    "                                     2 * torch.mm(embedding, torch.transpose(embedding, dim0=0, dim1=1))\n",
    "                                     + torch.transpose(embedding_norm, dim0=0, dim1=1)))\n",
    "        L_2nd = torch.sum(((adj_batch - t0) * b_mat) * ((adj_batch - t0) * b_mat))\n",
    "        return L_1st, self.alpha * L_2nd, L_1st + self.alpha * L_2nd\n",
    "\n",
    "    def savector(self, adj):\n",
    "        t0 = self.encode0(adj)\n",
    "        t0 = self.encode1(t0)\n",
    "        return t0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "782b8a4dbbf2a173"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SDNEDataset(Dataset):\n",
    "    def __init__(self, Adj, Node):\n",
    "        \"\"\"\n",
    "        Dataset Class for iteration\n",
    "        \n",
    "        :param Adj: adh matrix\n",
    "        :param Node: the number of nodes\n",
    "        \"\"\"\n",
    "        self.Adj = Adj\n",
    "        self.Node = Node\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        \n",
    "        :param index: the index of node\n",
    "        :return: the index of node \n",
    "        '''\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Node"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6d6cdb96c84f3c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"in_dim\": 1433,\n",
    "    \"out_dim\": 128,\n",
    "    \"dropout\": 0.5,\n",
    "    \"alpha\": 1e-2,\n",
    "    \"lr\": 0.001,\n",
    "    \"step_size\": 10,\n",
    "    \"gamma\": 0.9,\n",
    "    \"batch_size\": 100,\n",
    "    \"epoch\": 100,\n",
    "    \"beta\": 5,\n",
    "    \"nu1\": 1e-5,\n",
    "    \"nu2\": 1e-4\n",
    "}\n",
    "\n",
    "G, Adj, Node =  to_networkx(cora_data), cora_A, cora_A.shape[0]\n",
    "model = SDNE(Node, config[\"in_dim\"], config[\"out_dim\"], config[\"dropout\"], config[\"alpha\"])\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n",
    "Data = SDNEDataset(Adj, Node)\n",
    "Data = DataLoader(Data, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "iter_bar = tqdm(range(1, config[\"epoch\"] + 1))\n",
    "for epoch in iter_bar:\n",
    "    loss_sum, loss_L1, loss_L2, loss_reg = 0, 0, 0, 0\n",
    "    for index in Data:\n",
    "        adj_batch = Adj[index].to(device)\n",
    "        adj_mat = adj_batch[:, index].to(device)\n",
    "        b_mat = torch.ones_like(adj_batch).to(device)\n",
    "        b_mat[adj_batch != 0] = config[\"beta\"]\n",
    "        opt.zero_grad()\n",
    "        L_1st, L_2nd, L_all = model(adj_batch, adj_mat, b_mat)\n",
    "        L_reg = 0\n",
    "        for param in model.parameters():\n",
    "            L_reg += config[\"nu1\"] * torch.sum(torch.abs(param)) + config[\"nu2\"] * torch.sum(param * param)\n",
    "        Loss = L_all + L_reg\n",
    "        \n",
    "        # back propagation\n",
    "        Loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += Loss\n",
    "        loss_L1 += L_1st\n",
    "        loss_L2 += L_2nd\n",
    "        loss_reg += L_reg\n",
    "        \n",
    "        # update progress\n",
    "        scheduler.step(epoch)\n",
    "        iter_bar.set_postfix({\"loss_sum\": loss_sum.item(), \"loss_L1\": loss_L1.item(), \"loss_L2\": loss_L2.item(), \"loss_reg\": loss_reg.item()})\n",
    "        \n",
    "model.eval()\n",
    "embedding = model.savector(Adj.to(device))\n",
    "outVec = embedding.cpu().detach().numpy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec4b0e6d03e6a4d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outVec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57ed86ee14d2cdf3"
  },
  {
   "cell_type": "markdown",
   "id": "d423619974950c2f",
   "metadata": {},
   "source": [
    "# 5. Metapath2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fb98138781029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn.models import MetaPath2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fa05717e41beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metapath = [\n",
    "    ('author', 'writes', 'paper'),\n",
    "    ('paper', 'published_in', 'venue'),\n",
    "    ('venue', 'publishes', 'paper'),\n",
    "    ('paper', 'written_by', 'author'),\n",
    "]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MetaPath2Vec(aminer_data.edge_index_dict, embedding_dim=128,\n",
    "                     metapath=metapath, walk_length=50, context_size=7,\n",
    "                     walks_per_node=5, num_negative_samples=5,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3cd370b66e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_steps=100, eval_steps=2000):\n",
    "    '''\n",
    "    :param log_steps: the value of step for log \n",
    "    :param eval_steps: the value of step for evaluation\n",
    "    :return: \n",
    "    '''\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Loss: {total_loss / log_steps:.4f}'))\n",
    "            total_loss = 0\n",
    "\n",
    "        if (i + 1) % eval_steps == 0:\n",
    "            acc = test()\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Acc: {acc:.4f}'))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(train_ratio=0.1):\n",
    "    model.eval()\n",
    "\n",
    "    z = model('author', batch=aminer_data['author'].y_index.to(device))\n",
    "    y = aminer_data['author'].y\n",
    "\n",
    "    perm = torch.randperm(z.size(0))\n",
    "    train_perm = perm[:int(z.size(0) * train_ratio)]\n",
    "    test_perm = perm[int(z.size(0) * train_ratio):]\n",
    "\n",
    "    return model.test(z[train_perm], y[train_perm], z[test_perm], y[test_perm],\n",
    "                      max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6388b6e8eea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. HIN2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be3cdb4bfc05548"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51df19a116b63ecd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def binary_reg(x: torch.Tensor):\n",
    "    # forward: f(x) = (x>=0)\n",
    "    # backward: f(x) = sigmoid\n",
    "    a = torch.sigmoid(x)\n",
    "    b = a.detach()\n",
    "    c = (x.detach() >= 0).float()\n",
    "    return a - b + c\n",
    "\n",
    "class HIN2vec(nn.Module):\n",
    "\n",
    "    def __init__(self, node_size, path_size, embed_dim, sigmoid_reg=False, r=True):\n",
    "        '''\n",
    "        \n",
    "        :param node_size: the number of nodes\n",
    "        :param path_size: the length of path\n",
    "        :param embed_dim: the size of dim for embedding\n",
    "        :param sigmoid_reg: the argument for setting sigmoid\n",
    "        :param r: the argument for pretrained loss (if True, model will be trained with random embedding table)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.reg = torch.sigmoid if sigmoid_reg else binary_reg\n",
    "\n",
    "        self.__initialize_model(node_size, path_size, embed_dim, r)\n",
    "\n",
    "    def __initialize_model(self, node_size, path_size, embed_dim, r):\n",
    "        self.start_embeds = nn.Embedding(node_size, embed_dim)\n",
    "        self.end_embeds = self.start_embeds if r else nn.Embedding(node_size, embed_dim)\n",
    "\n",
    "        self.path_embeds = nn.Embedding(path_size, embed_dim)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim, 1),\n",
    "        #     nn.Sigmoid(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, start_node: torch.LongTensor, end_node: torch.LongTensor, path: torch.LongTensor):\n",
    "        '''\n",
    "        \n",
    "        :param start_node: the input of start node\n",
    "        :param end_node:  the input of end node\n",
    "        :param path:  the input of path\n",
    "        :return: the output of model\n",
    "        '''\n",
    "        # assert start_node.dim() == 1  # shape = (batch_size,)\n",
    "\n",
    "        s = self.start_embeds(start_node)  # (batch_size, embed_size)\n",
    "        e = self.end_embeds(end_node)\n",
    "        p = self.path_embeds(path)\n",
    "        p = self.reg(p)\n",
    "\n",
    "        agg = torch.mul(s, e)\n",
    "        agg = torch.mul(agg, p)\n",
    "        # agg = F.sigmoid(agg)\n",
    "        # output = self.classifier(agg)\n",
    "\n",
    "        output = torch.sigmoid(torch.sum(agg, axis=1))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader: DataLoader, optimizer, loss_function, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data[:, 0], data[:, 1], data[:, 2])\n",
    "        loss = loss_function(output.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % log_interval == 0:\n",
    "            print(f'\\rTrain Epoch: {epoch} '\n",
    "                  f'[{idx * len(data)}/{len(train_loader.dataset)} ({100. * idx / len(train_loader):.3f}%)]\\t'\n",
    "                  f'Loss: {loss.item():.3f}\\t\\t',\n",
    "                  # f'data = {data}\\t target = {target}',\n",
    "                  end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "class NSTrainSet(Dataset):\n",
    "    def __init__(self, sample, node_size, neg=5):\n",
    "        \"\"\"\n",
    "        :param node_size: the number of nodes\n",
    "        :param neg: the number of negative samples\n",
    "        :param sample: return value of HIN.sample()，(start_node, end_node, path_id)\n",
    "        \"\"\"\n",
    "\n",
    "        print('init training dataset...')\n",
    "\n",
    "        l = len(sample)\n",
    "\n",
    "        x = np.tile(sample, (neg + 1, 1))\n",
    "        y = np.zeros(l * (1 + neg))\n",
    "        y[:l] = 1\n",
    "\n",
    "        # x[l:, 2] = np.random.randint(0, path_size - 1, (l * neg,))\n",
    "        x[l:, 1] = np.random.randint(0, node_size - 1, (l * neg,))\n",
    "\n",
    "        self.x = torch.LongTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.length = len(x)\n",
    "\n",
    "        print('finished')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da80ba6183e51eef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HIN:\n",
    "    \"\"\"\n",
    "    Class to generate vertex sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, window=None):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.node_size = 0\n",
    "        self._path_size = 0\n",
    "\n",
    "        def new_id():\n",
    "            i = self.node_size\n",
    "            self.node_size += 1\n",
    "            return i\n",
    "\n",
    "        self._node2id = defaultdict(new_id)\n",
    "        self._id2type = {}\n",
    "        self._window = window\n",
    "        self._node_types = set()\n",
    "        self._path2id = None\n",
    "        self._id2path = None\n",
    "        self._id2node = None\n",
    "\n",
    "    @property\n",
    "    def id2node(self):\n",
    "        return self._id2node\n",
    "\n",
    "    @property\n",
    "    def id2path(self):\n",
    "        return self._id2path\n",
    "\n",
    "    @property\n",
    "    def window(self):\n",
    "        return self._window\n",
    "\n",
    "    @window.setter\n",
    "    def window(self, val):\n",
    "        if not self._window:\n",
    "            self._window = val\n",
    "        else:\n",
    "            raise ValueError(\"window error\")\n",
    "\n",
    "    @property\n",
    "    def path_size(self):\n",
    "        if not self._path_size:\n",
    "            raise ValueError(\"run sample() first to count path size\")\n",
    "        return self._path_size\n",
    "\n",
    "    def add_edge(self, source_node, source_class, dest_node, dest_class, edge_class, weight):\n",
    "        i = self._node2id[source_node]\n",
    "        j = self._node2id[dest_node]\n",
    "        self._id2type[i] = source_class\n",
    "        self._id2type[j] = dest_class\n",
    "        self._node_types.add(source_class)\n",
    "        self._node_types.add(dest_class)\n",
    "        self.graph.add_edge(i, j, weight=weight)\n",
    "\n",
    "    def small_walk(self, start_node, length):\n",
    "        walk = [start_node]\n",
    "        for i in range(1, length):\n",
    "            if next(nx.neighbors(self.graph, walk[-1]), None) is None:\n",
    "                break\n",
    "            cur_node = walk[-1]\n",
    "            nodes = list(nx.neighbors(self.graph, cur_node))\n",
    "            weights = [self.graph[cur_node][i]['weight'] for i in nodes]\n",
    "            s = sum(weights)\n",
    "            weights = [i/s for i in weights]\n",
    "            walk += random.choices(nodes, weights, k=1)\n",
    "        return walk\n",
    "\n",
    "    def do_walks(self, length):\n",
    "        for start_node in range(self.node_size):\n",
    "            yield self.small_walk(start_node, length)\n",
    "\n",
    "    def sample(self, length, n_repeat):\n",
    "        \"\"\"\n",
    "        :param length: the length of path\n",
    "        :param n_repeat: the number of repeat\n",
    "        :return: (start_id, end_id, edge_type)\n",
    "        \"\"\"\n",
    "        if not self.window:\n",
    "            raise ValueError(\"window not set\")\n",
    "\n",
    "        if not self._path2id:\n",
    "            self._path2id = {}\n",
    "            path_id = 0\n",
    "            for w in range(1, self._window + 1):\n",
    "                for i in product(self._node_types, repeat=w + 1):\n",
    "                    self._path2id[i] = path_id\n",
    "                    path_id += 1\n",
    "\n",
    "            self._path_size = len(self._path2id)\n",
    "            self._id2node = {v: k for k, v in self._node2id.items()}\n",
    "            self._id2path = {v: k for k, v in self._path2id.items()}\n",
    "\n",
    "        samples = []\n",
    "\n",
    "        for repeat in range(n_repeat):\n",
    "            for walk in self.do_walks(length):\n",
    "                cur_len = 0\n",
    "                for i, node_id in enumerate(walk):\n",
    "                    cur_len = min(cur_len + 1, self._window + 1)  # 当window=n的时候，最长路径有n+1个节点\n",
    "                    if cur_len >= 2:\n",
    "                        for path_length in range(1, cur_len):\n",
    "                            sample = (walk[i - path_length], walk[i],\n",
    "                                      self._path2id[tuple([self._id2type[t] for t in walk[i - path_length:i + 1]])])\n",
    "                            # print(tuple([self.id2type[t] for t in walk[i-path_length:i + 1]]))\n",
    "                            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print(f'size = {self.node_size}')\n",
    "\n",
    "\n",
    "def load_a_HIN_from_pandas(edges, print_graph=False):\n",
    "    \"\"\"\n",
    "    edges = list(pd.df)\n",
    "    \"\"\"\n",
    "\n",
    "    def reverse(df):\n",
    "        \"\"\"\n",
    "        reverse source & dest\n",
    "        \"\"\"\n",
    "        df = df.rename({'source_node': 'dest_node', 'dest_node': 'source_node',\n",
    "                        'source_class': 'dest_class', 'dest_class': 'source_class'},\n",
    "                       axis=1)\n",
    "        # reverse edge_class\n",
    "        df.edge_class = df.edge_class.map(lambda x: '-'.join(reversed(x.split('-'))))\n",
    "        return df\n",
    "\n",
    "    print('load graph from edges...')\n",
    "    g = HIN()\n",
    "    if isinstance(edges, list):\n",
    "        edges = pd.concat(edges, sort=False)\n",
    "    # edges = edges.append(reverse(edges), sort=False, ignore_index=True)\n",
    "    edges = pd.concat([edges, reverse(edges)], ignore_index=True, sort=False)\n",
    "\n",
    "    for index, row in edges.iterrows():\n",
    "        g.add_edge(row['source_node'], row['source_class'],\n",
    "                   row['dest_node'], row['dest_class'], row['edge_class'],\n",
    "                   row['weight'])\n",
    "    if print_graph:\n",
    "        g.print_statistics()\n",
    "    print('finish loading graph!')\n",
    "    return g"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfdc67cbd1b7e851"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set method parameters\n",
    "window = 4\n",
    "walk = 10\n",
    "walk_length = 300\n",
    "embed_size = 100\n",
    "neg = 5\n",
    "sigmoid_reg = True \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device = {device}')\n",
    "\n",
    "# set dataset [PLEASE USE YOUR OWN DATASET TO REPLACE THIS]\n",
    "demo_edge = pd.read_csv('./sample_graph_df.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "435b2980771d529"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "demo_edge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b98a129d7ae40c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edges = [demo_edge]\n",
    "\n",
    "print('finish loading edges')\n",
    "\n",
    "# init HIN\n",
    "hin = load_a_HIN_from_pandas(edges)\n",
    "hin.window = window\n",
    "\n",
    "dataset = NSTrainSet(hin.sample(walk_length, walk), hin.node_size, neg=neg)\n",
    "\n",
    "hin2vec = HIN2vec(hin.node_size, hin.path_size, embed_size, sigmoid_reg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74c866260a9518a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "n_epoch = 1\n",
    "batch_size = 20\n",
    "log_interval = 200\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(hin2vec.parameters())\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train(log_interval, hin2vec, device, data_loader, optimizer, loss_function, epoch)\n",
    "\n",
    "torch.save(hin2vec, 'hin2vec.pt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d6380825636b8c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set output parameters [the output file is a bit different from the original code.]\n",
    "node_vec_fname = 'node_vec.txt'\n",
    "path_vec_fname = \"path_2vec.txt\"\n",
    "    \n",
    "print(f'saving node embedding vectors to {node_vec_fname}...')\n",
    "node_embeds = pd.DataFrame(hin2vec.start_embeds.weight.data.cpu().numpy())\n",
    "node_embeds.rename(hin.id2node).to_csv(node_vec_fname, sep=' ')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794479c275a0cee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "node_embeds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762edbab14fbbeed"
  },
  {
   "cell_type": "markdown",
   "id": "65fb0b725aece614",
   "metadata": {},
   "source": [
    "# 7. WL-relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d18cc3298da7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wlk_relabel(g,h):\n",
    "    '''\n",
    "    Perform node relabeling (coloring) according 1-d WL relabeling process (refer Shervashidze et al (2009) paper)\n",
    "    :param g: networkx graph\n",
    "    :param h: height of WL kernel\n",
    "    :return: relabeled graph\n",
    "    '''\n",
    "    for i in range(len(g.nodes)):\n",
    "        g.nodes[i]['relabel'] = {}\n",
    "        \n",
    "    for i in range(0,h+1): #xrange returns [min,max)\n",
    "        for n in range(len(g.nodes)):\n",
    "            # degree_prefix = 'D' + str(i)\n",
    "            degree_prefix = ''\n",
    "            if 0 == i:\n",
    "                g.nodes[n]['relabel'][0] = degree_prefix + str(g.nodes[n]['label']).strip() + degree_prefix\n",
    "            else:\n",
    "                nei_labels = [g.nodes[nei]['relabel'][i-1] for nei in nx.all_neighbors(g,n)]\n",
    "                nei_labels.sort()\n",
    "                sorted_nei_labels = (','*i).join(nei_labels)\n",
    "\n",
    "                current_in_relabel = g.nodes[n]['relabel'][i-1] +'#'*i+ sorted_nei_labels\n",
    "                g.nodes[n]['relabel'][i] = degree_prefix + current_in_relabel.strip() + degree_prefix\n",
    "    return g #relabled graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194bea93078473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_labels(G):\n",
    "    for i in range(len(G.nodes)):\n",
    "        G.nodes[i]['label'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aefaf26ea60755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G):\n",
    "    clubs = []  # list to populate with labels\n",
    "    for n in G.nodes:\n",
    "        c = G.nodes[n]['relabel']\n",
    "        print(f'Node ID {n}: colour: {c}')\n",
    "        col = 0\n",
    "        if c=='1':\n",
    "            clubs.append('green')\n",
    "        elif c =='2':\n",
    "            clubs.append('red')\n",
    "        else:\n",
    "            clubs.append('gray')\n",
    "    pos = nx.spring_layout(G, seed=42) # To be able to recreate the graph layout\n",
    "    nx.draw_networkx(G, pos=pos, node_color = clubs) # Plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2825ef09443eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph([\n",
    "    (0, 1),\n",
    "    (1 ,2), \n",
    "    (1 ,3),\n",
    "    (1 ,5),\n",
    "    (2 ,3),\n",
    "    (2 ,4),\n",
    "    (2 ,6),\n",
    "    (3 ,4),\n",
    "    (3 ,5),\n",
    "    (4 ,7),\n",
    "    (4 ,8),\n",
    "    (6 ,7)])\n",
    "fill_labels(G)\n",
    "nx.draw(G, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b9ecb9bd7cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled_G = wlk_relabel(G, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6460c8eb2f74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(relabeled_G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
