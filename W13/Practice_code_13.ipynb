{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv-tHPvR-JKa"
   },
   "source": [
    "### 1. GT (Graph Transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8wIJZQqODy-7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (5.2.0)\n",
      "installing ... \n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "installed! torch 2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (1.3.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (1.3.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (2.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (1.26.18)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from ogb) (0.2.2)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from outdated>=0.2.0->ogb) (68.0.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch>=1.6.0->ogb) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from tqdm>=4.29.0->ogb) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
      "installed!\n",
      "DGL installed!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "!pip install chardet\n",
    "print(\"installing ... \")\n",
    "!conda install -y pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "print(\"installed! torch 2.0.0\")\n",
    "!conda install -c dglteam/label/cu117 dgl\n",
    "\n",
    "\n",
    "\n",
    "# !pip install torchvision \n",
    "# !pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html\n",
    "!pip install ogb\n",
    "\n",
    "print(\"installed!\")\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    installed = True\n",
    "except ImportError:\n",
    "    installed = False\n",
    "print(\"DGL installed!\" if installed else \"Failed to install DGL!\")\n",
    "\n",
    "#!pip install  torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
    "# pip install chardet\n",
    "# Uncomment below to install required packages. If the CUDA version is not 11.8,\n",
    "# check the https://www.dgl.ai/pages/start.html to find the supported CUDA\n",
    "# version and corresponding command to install DGL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOpFdtLI-JKb"
   },
   "source": [
    "### Sparse Multi-head Attention\n",
    "\n",
    "Recall the all-pairs scaled-dot-product attention mechanism in vanillar Transformer:\n",
    "\n",
    "$$\\text{Attn}=\\text{softmax}(\\dfrac{QK^T} {\\sqrt{d}})V,$$\n",
    "\n",
    "The graph transformer (GT) model employs a Sparse Multi-head Attention block:\n",
    "\n",
    "$$\\text{SparseAttn}(Q, K, V, A) = \\text{softmax}(\\frac{(QK^T) \\circ A}{\\sqrt{d}})V,$$\n",
    "\n",
    "where $Q, K, V ∈\\mathbb{R}^{N\\times d}$ are query feature, key feature, and value feature, respectively. $A\\in[0,1]^{N\\times N}$ is the adjacency matrix of the input graph. $(QK^T)\\circ A$ means that the multiplication of query matrix and key matrix is followed by a Hadamard product (or element-wise multiplication) with the sparse adjacency matrix as illustrated in the figure below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1OgMAewLR3Z1vz5y4J8aPRSeaU3g8iQfX\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dh7zc5v0-JKb"
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.sparse as dglsp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import AsGraphPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from ogb.graphproppred import collate_dgl, DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SparseMHA(nn.Module):\n",
    "    \"\"\"Sparse Multi-head Attention Module\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        N = len(h)\n",
    "        # [N, dh, nh]\n",
    "        q = self.q_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        q *= self.scaling\n",
    "        # [N, dh, nh]\n",
    "        k = self.k_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        # [N, dh, nh]\n",
    "        v = self.v_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "\n",
    "        ######################################################################\n",
    "        # (HIGHLIGHT) Compute the multi-head attention with Sparse Matrix API\n",
    "        ######################################################################\n",
    "        attn = dglsp.bsddmm(A, q, k.transpose(1, 0))  # (sparse) [N, N, nh]\n",
    "        # Sparse softmax by default applies on the last sparse dimension.\n",
    "        attn = attn.softmax()  # (sparse) [N, N, nh]\n",
    "        out = dglsp.bspmm(attn, v)  # [N, dh, nh]\n",
    "\n",
    "        return self.out_proj(out.reshape(N, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Fm6Lrx-JKc"
   },
   "source": [
    "## Graph Transformer Layer\n",
    "\n",
    "The GT layer is composed of Multi-head Attention, Batch Norm, and Feed-forward Network, connected by residual links as in vanilla transformer.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1cm-Ijw7bUQIOkoTKn5MQ3m4-66JqCsMz\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M6h7JVWT-JKd"
   },
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    \"\"\"Graph Transformer Layer\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.MHA = SparseMHA(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.FFN1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.FFN2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        h1 = h\n",
    "        h = self.MHA(A, h)\n",
    "        h = self.batchnorm1(h + h1)\n",
    "\n",
    "        h2 = h\n",
    "        h = self.FFN2(F.relu(self.FFN1(h)))\n",
    "        h = h2 + h\n",
    "\n",
    "        return self.batchnorm2(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t40DhVjI-JKd"
   },
   "source": [
    "## Graph Transformer Model\n",
    "\n",
    "The GT model is constructed by stacking GT layers. The input positional encoding of vanilla transformer is replaced with Laplacian positional encoding [(Dwivedi et al. 2020)](https://arxiv.org/abs/2003.00982). For the graph-level prediction task, an extra pooler is stacked on top of GT layers to aggregate node feature of the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UrjvEBrF-JKe"
   },
   "outputs": [],
   "source": [
    "class GTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size,\n",
    "        hidden_size=80,\n",
    "        pos_enc_size=2,\n",
    "        num_layers=8,\n",
    "        num_heads=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.atom_encoder = AtomEncoder(hidden_size)\n",
    "        self.pos_linear = nn.Linear(pos_enc_size, hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GTLayer(hidden_size, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.pooler = dglnn.SumPooling()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 4, out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, g, X, pos_enc):\n",
    "        indices = torch.stack(g.edges())\n",
    "        N = g.num_nodes()\n",
    "        A = dglsp.spmatrix(indices, shape=(N, N))\n",
    "        h = self.atom_encoder(X) + self.pos_linear(pos_enc)\n",
    "        for layer in self.layers:\n",
    "            h = layer(A, h)\n",
    "        h = self.pooler(g, h)\n",
    "\n",
    "        return self.predictor(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdrPU18I-JKe"
   },
   "source": [
    "### Training\n",
    "\n",
    "We train the GT model on [ogbg-molhiv](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol) benchmark. \n",
    "The Laplacian positional encoding of each graph is pre-computed as part of the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V41i0w-9-JKe",
    "outputId": "15343d1a-a32d-4677-d053-d9da96910f43"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, evaluator, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batched_g, labels in dataloader:\n",
    "        batched_g, labels = batched_g.to(device), labels.to(device)\n",
    "        y_hat = model(batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"])\n",
    "        y_true.append(labels.view(y_hat.shape).detach().cpu())\n",
    "        y_pred.append(y_hat.detach().cpu())\n",
    "    y_true = torch.cat(y_true, dim=0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "    return evaluator.eval(input_dict)[\"rocauc\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataset, evaluator, device):\n",
    "    train_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.train_idx],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_dgl,\n",
    "    )\n",
    "    valid_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.val_idx], batch_size=256, collate_fn=collate_dgl\n",
    "    )\n",
    "    test_dataloader = GraphDataLoader(\n",
    "        dataset[dataset.test_idx], batch_size=256, collate_fn=collate_dgl\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 5\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=num_epochs, gamma=0.5\n",
    "    )\n",
    "    loss_fcn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batched_g, labels in train_dataloader:\n",
    "            batched_g, labels = batched_g.to(device), labels.to(device)\n",
    "            logits = model(\n",
    "                batched_g, batched_g.ndata[\"feat\"], batched_g.ndata[\"PE\"]\n",
    "            )\n",
    "            loss = loss_fcn(logits, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        val_metric = evaluate(model, valid_dataloader, evaluator, device)\n",
    "        test_metric = evaluate(model, test_dataloader, evaluator, device)\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, Loss: {avg_loss:.4f}, \"\n",
    "            f\"Val: {val_metric:.4f}, Test: {test_metric:.4f}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Laplacian PE:   0%|                                                                 | 0/4000 [00:00<?, ?it/s]C:\\Users\\user\\anaconda3\\envs\\GNN16\\lib\\site-packages\\dgl\\transforms\\functional.py:3711: DGLWarning: dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\n",
      "  dgl_warning(\"dgl.laplacian_pe will be deprecated. Use dgl.lap_pe please.\")\n",
      "Computing Laplacian PE:   0%|▏                                                      | 15/4000 [00:00<00:26, 149.88it/s]C:\\Users\\user\\anaconda3\\envs\\GNN16\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:53: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Copy.cpp:276.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n",
      "Computing Laplacian PE: 100%|█████████████████████████████████████████████████████| 4000/4000 [00:12<00:00, 327.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.4448, Val: 0.2640, Test: 0.3369\n",
      "Epoch: 001, Loss: 0.2137, Val: 0.2971, Test: 0.3529\n",
      "Epoch: 002, Loss: 0.1792, Val: 0.4039, Test: 0.4484\n",
      "Epoch: 003, Loss: 0.1579, Val: 0.5109, Test: 0.4307\n",
      "Epoch: 004, Loss: 0.1400, Val: 0.5626, Test: 0.4903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training device.\n",
    "dev = torch.device(\"cpu\")\n",
    "#Be sure to install DGL with CUDA support.\n",
    "#dev = torch.device(\"cuda:0\")\n",
    "\n",
    "# Load dataset.\n",
    "pos_enc_size = 8\n",
    "dataset = AsGraphPredDataset(\n",
    "    DglGraphPropPredDataset(\"ogbg-molhiv\", \"./data/OGB\")\n",
    ")\n",
    "evaluator = Evaluator(\"ogbg-molhiv\")\n",
    "\n",
    "# Down sample the dataset to make the tutorial run faster.\n",
    "import random\n",
    "random.seed(42)\n",
    "train_size = len(dataset.train_idx)\n",
    "val_size = len(dataset.val_idx)\n",
    "test_size = len(dataset.test_idx)\n",
    "dataset.train_idx = dataset.train_idx[\n",
    "    torch.LongTensor(random.sample(range(train_size), 2000))\n",
    "]\n",
    "dataset.val_idx = dataset.val_idx[\n",
    "    torch.LongTensor(random.sample(range(val_size), 1000))\n",
    "]\n",
    "dataset.test_idx = dataset.test_idx[\n",
    "    torch.LongTensor(random.sample(range(test_size), 1000))\n",
    "]\n",
    "\n",
    "# Laplacian positional encoding.\n",
    "indices = torch.cat([dataset.train_idx, dataset.val_idx, dataset.test_idx])\n",
    "for idx in tqdm(indices, desc=\"Computing Laplacian PE\"):\n",
    "    g, _ = dataset[idx]\n",
    "    g.ndata[\"PE\"] = dgl.laplacian_pe(g, k=pos_enc_size, padding=True)\n",
    "\n",
    "# Create model.\n",
    "out_size = dataset.num_tasks\n",
    "model = GTModel(out_size=out_size, pos_enc_size=pos_enc_size).to(dev)\n",
    "\n",
    "# Kick off training.\n",
    "train(model, dataset, evaluator, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph GPS\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*QKN2j0vBNS8fF-W2EuW5NQ.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torchversion = torch.__version__\n",
    "print(torchversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from jinja2->torch_geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+$11.7.html\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (0.6.18)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\user\\anaconda3\\envs\\gnn16\\lib\\site-packages (from scipy->torch-sparse) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n",
    "!pip install torch_geometric\n",
    "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+${11.7}.html\n",
    "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{2.0.0}.html\n",
    "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{2.0.0}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GINEConv, GPSConv, global_add_pool\n",
    "from torch_geometric.nn.attention import PerformerAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ZINC-pe1'\n",
    "dev = torch.device(\"cuda:0\")\n",
    "transform = T.AddRandomWalkPE(walk_length=20, attr_name='pe')\n",
    "train_dataset = ZINC(path, subset=True, split='train')\n",
    "val_dataset = ZINC(path, subset=True, split='val')\n",
    "test_dataset = ZINC(path, subset=True, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# #parser.add_argument( default='multihead')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "#*** Importance:\n",
    "# Copy file add_positional_encoding.py \n",
    "# to anaconda3\\envs\\your envs name\\lib\\site-packages\\torch_geometric\\transforms\\add_positional_encoding.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPS(torch.nn.Module):\n",
    "    def __init__(self, channels: int, pe_dim: int, num_layers: int,\n",
    "                 attn_type: str, attn_kwargs: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.node_emb = Embedding(28, channels )\n",
    "        self.pe_lin = Linear(20, pe_dim)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Embedding(4, channels)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(channels, channels),\n",
    "                ReLU(),\n",
    "                Linear(channels, channels),\n",
    "            )\n",
    "            conv = GPSConv(channels, GINEConv(nn), heads=4,\n",
    "                           attn_type=attn_type, attn_kwargs=attn_kwargs)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // 2),\n",
    "            ReLU(),\n",
    "            Linear(channels // 2, channels // 4),\n",
    "            ReLU(),\n",
    "            Linear(channels // 4, 1),\n",
    "        )\n",
    "        self.redraw_projection = RedrawProjection(\n",
    "            self.convs,\n",
    "            redraw_interval=1000 if attn_type == 'performer' else None)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_emb(x.squeeze(-1))\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, batch, edge_attr=edge_attr)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class RedrawProjection:\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 redraw_interval: Optional[int] = None):\n",
    "        self.model = model\n",
    "        self.redraw_interval = redraw_interval\n",
    "        self.num_last_redraw = 0\n",
    "\n",
    "    def redraw_projections(self):\n",
    "        if not self.model.training or self.redraw_interval is None:\n",
    "            return\n",
    "        if self.num_last_redraw >= self.redraw_interval:\n",
    "            fast_attentions = [\n",
    "                module for module in self.model.modules()\n",
    "                if isinstance(module, PerformerAttention)\n",
    "            ]\n",
    "            for fast_attention in fast_attentions:\n",
    "                fast_attention.redraw_projection_matrix()\n",
    "            self.num_last_redraw = 0\n",
    "            return\n",
    "        self.num_last_redraw += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attn_kwargs = {'dropout': 0.5}\n",
    "model = GPS(channels=64, pe_dim=8, num_layers=10, attn_type='performer',\n",
    "            attn_kwargs=attn_kwargs).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.redraw_projection.redraw_projections()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x,  data.edge_index, data.edge_attr,\n",
    "                    data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5863, Val: 0.6312, Test: 0.6813\n",
      "Epoch: 02, Loss: 0.5363, Val: 0.5040, Test: 0.5454\n",
      "Epoch: 03, Loss: 0.5318, Val: 0.6812, Test: 0.7000\n",
      "Epoch: 04, Loss: 0.5131, Val: 0.5102, Test: 0.5310\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    loss = train()\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
